[![GitHub watchers](https://img.shields.io/badge/tulip--lab-Pattern--Classification-brightgreen)](../README.md)
[![GitHub watchers](https://img.shields.io/badge/Module-Deep--Learning-orange)](README.md)

# Deep Learning

`Deep learning` is a subfield of machine learning that focuses on training artificial neural networks with multiple layers, often referred to as deep neural networks. Deep learning models are designed to automatically learn hierarchical representations of data, enabling them to capture complex patterns and make high-level abstractions. Deep learning models are typically trained using large amounts of labeled data through a process called `backpropagation`, which involves iteratively adjusting the model's internal parameters (weights and biases) to minimize the difference between predicted outputs and actual targets. This optimization is achieved using gradient descent algorithms, such as `stochastic gradient descent` (SGD) or its variants, which update the model's parameters based on the gradients of the loss function with respect to those parameters.

- `Convolutional Neural Networks` (CNNs): CNNs are a type of deep learning model particularly effective for image and video processing tasks. They are designed to automatically learn and extract hierarchical representations of visual features from input data. CNNs consist of convolutional layers that apply filters to input data, pooling layers that downsample feature maps, and fully connected layers for classification or regression. CNNs have been highly successful in various applications, including image recognition, object detection, and image generation.

- `Recurrent Neural Networks` (RNNs): RNNs are deep learning models specifically designed for sequential data, such as natural language processing and speech recognition. Unlike feedforward neural networks, RNNs have feedback connections that allow them to capture information from previous time steps. This enables RNNs to model sequences and handle variable-length inputs. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are popular variants of RNNs that can effectively capture long-range dependencies and mitigate the vanishing gradient problem.

Deep learning has revolutionized various fields, including computer vision, natural language processing, speech recognition, and robotics. It has achieved remarkable performance in tasks such as image classification, object detection, machine translation, and speech synthesis. The ability to automatically learn representations and hierarchies of data makes deep learning models highly adaptable and capable of tackling complex real-world problems.

## Lecture Slides Handouts

- [Lecture A: Deep Learning (1)](https://github.com/tulip-lab/handouts/blob/main/PR/PR-S09A.pdf)
- [Lecture B: Deep Learning (2)](https://github.com/tulip-lab/handouts/blob/main/PR/PR-S09B.pdf) 
- [Lecture C: Convolutional Neural Networks](https://github.com/tulip-lab/handouts/blob/main/PR/PR-S09C.pdf) 
- [Lecture D: Recurrent Neural Networks](https://github.com/tulip-lab/handouts/blob/main/PR/PR-S09D.pdf) 


